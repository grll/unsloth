{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Qwen2 patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2070. Max memory: 7.607 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\"unsloth/Qwen2-0.5B-Instruct-bnb-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Qwen2 patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2070. Max memory: 7.607 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Qwen2-0.5B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name        = \"Qwen/Qwen2-7B\",\n",
    "max_seq_length    = 4096,\n",
    "dtype             = None,\n",
    "load_in_4bit      = True,\n",
    "token             = None,\n",
    "device_map        = \"sequential\",\n",
    "rope_scaling      = None, # Qwen2 does not support RoPE scaling\n",
    "fix_tokenizer     = True,\n",
    "model_patcher     = None,\n",
    "tokenizer_name    = None,\n",
    "trust_remote_code = False,\n",
    "\n",
    "FastQwen2Model.from_pretrained(\n",
    "    model_name        = model_name,\n",
    "    max_seq_length    = max_seq_length,\n",
    "    dtype             = dtype,\n",
    "    load_in_4bit      = load_in_4bit,\n",
    "    token             = token,\n",
    "    device_map        = device_map,\n",
    "    rope_scaling      = rope_scaling,\n",
    "    fix_tokenizer     = fix_tokenizer,\n",
    "    model_patcher     = dispatch_model,\n",
    "    tokenizer_name    = tokenizer_name,\n",
    "    trust_remote_code = trust_remote_code,\n",
    "    revision          = revision if not is_peft else None,\n",
    "    *args, **kwargs,\n",
    ")\n",
    "\n",
    "# if load_in_4bit:\n",
    "#     # Fix up bitsandbytes config\n",
    "#     quantization_config = \\\n",
    "#     {\n",
    "#         # Sometimes torch_dtype is not a string!!\n",
    "#         \"bnb_4bit_compute_dtype\"           : model.config.to_dict()[\"torch_dtype\"],\n",
    "#         \"bnb_4bit_quant_type\"              : \"nf4\",\n",
    "#         \"bnb_4bit_use_double_quant\"        : True,\n",
    "#         \"llm_int8_enable_fp32_cpu_offload\" : False,\n",
    "#         \"llm_int8_has_fp16_weight\"         : False,\n",
    "#         \"llm_int8_skip_modules\"            : None,\n",
    "#         \"llm_int8_threshold\"               : 6.0,\n",
    "#         \"load_in_4bit\"                     : True,\n",
    "#         \"load_in_8bit\"                     : False,\n",
    "#         \"quant_method\"                     : \"bitsandbytes\",\n",
    "#     }\n",
    "#     model.config.update({\"quantization_config\" : quantization_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  \"get_ipython().run_line_magic('load', '')\",\n",
       "  \"get_ipython().run_line_magic('load_ext', 'autoreload')\\nget_ipython().run_line_magic('autoreload', '1')\",\n",
       "  'from unsloth.models.qwen2 import Qwen2',\n",
       "  'from unsloth.models.qwen2 import FastQwen2Model',\n",
       "  'globals()'],\n",
       " '_oh': {},\n",
       " '_dh': [PosixPath('/home/guillaume/Projects/python/unsloth/notebooks')],\n",
       " 'In': ['',\n",
       "  \"get_ipython().run_line_magic('load', '')\",\n",
       "  \"get_ipython().run_line_magic('load_ext', 'autoreload')\\nget_ipython().run_line_magic('autoreload', '1')\",\n",
       "  'from unsloth.models.qwen2 import Qwen2',\n",
       "  'from unsloth.models.qwen2 import FastQwen2Model',\n",
       "  'globals()'],\n",
       " 'Out': {},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f8e6536e440>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f8e6536f0a0>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f8e6536f0a0>,\n",
       " 'open': <function io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       " '_': '',\n",
       " '__': '',\n",
       " '___': '',\n",
       " '__vsc_ipynb_file__': '/home/guillaume/Projects/python/unsloth/notebooks/01_qwen_for_seq_cls.ipynb',\n",
       " '_i': 'from unsloth.models.qwen2 import FastQwen2Model',\n",
       " '_ii': 'from unsloth.models.qwen2 import Qwen2',\n",
       " '_iii': '%load_ext autoreload\\n%autoreload 1',\n",
       " '_i1': '%load',\n",
       " '_i2': '%load_ext autoreload\\n%autoreload 1',\n",
       " '_i3': 'from unsloth.models.qwen2 import Qwen2',\n",
       " '_i4': 'from unsloth.models.qwen2 import FastQwen2Model',\n",
       " 'FastQwen2Model': unsloth.models.qwen2.FastQwen2Model,\n",
       " '_i5': 'globals()'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Qwen2 patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2070. Max memory: 7.607 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/.venv/ml/lib/python3.10/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Qwen2-0.5B-Instruct-bnb-4bit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Tuple, Union, List\n",
    "import xformers.ops.fmha as xformers\n",
    "from unsloth.models.gemma2 import Gemma2Model_fast_forward_inference\n",
    "from unsloth.models.llama import LlamaModel_fast_forward_inference\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, SequenceClassifierOutputWithPast\n",
    "\n",
    "\n",
    "# override def _CausalLM_fast_forward(\n",
    "def _transformer_fast_forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    "    *args, **kwargs,\n",
    ") -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "    \n",
    "    if past_key_values is not None:\n",
    "        # Gemma2Model_fast_forward_inference for gemma 2\n",
    "        # for qwen2 this is LlamaModel_fast_forward_inference\n",
    "        outputs = LlamaModel_fast_forward_inference(\n",
    "            self,\n",
    "            input_ids,\n",
    "            past_key_values,\n",
    "            position_ids = position_ids,\n",
    "            attention_mask = attention_mask,\n",
    "        )\n",
    "    else:\n",
    "        causal_mask = xformers.attn_bias.LowerTriangularMask()\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        self.model._has_no_labels = labels is None\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            causal_mask=causal_mask,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('model',\n",
       "               Qwen2Model(\n",
       "                 (embed_tokens): Embedding(151936, 896)\n",
       "                 (layers): ModuleList(\n",
       "                   (0-23): 24 x Qwen2DecoderLayer(\n",
       "                     (self_attn): Qwen2Attention(\n",
       "                       (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "                       (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "                       (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "                       (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "                       (rotary_emb): LlamaRotaryEmbedding()\n",
       "                     )\n",
       "                     (mlp): Qwen2MLP(\n",
       "                       (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "                       (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "                       (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "                       (act_fn): SiLU()\n",
       "                     )\n",
       "                     (input_layernorm): Qwen2RMSNorm()\n",
       "                     (post_attention_layernorm): Qwen2RMSNorm()\n",
       "                   )\n",
       "                 )\n",
       "                 (norm): Qwen2RMSNorm()\n",
       "               )),\n",
       "              ('lm_head',\n",
       "               Linear(in_features=896, out_features=151936, bias=False))]),\n",
       " 'config': Qwen2Config {\n",
       "   \"_name_or_path\": \"unsloth/Qwen2-0.5B-Instruct-bnb-4bit\",\n",
       "   \"architectures\": [\n",
       "     \"Qwen2ForCausalLM\"\n",
       "   ],\n",
       "   \"attention_dropout\": 0.0,\n",
       "   \"bos_token_id\": 151643,\n",
       "   \"eos_token_id\": 151645,\n",
       "   \"hidden_act\": \"silu\",\n",
       "   \"hidden_size\": 896,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 4864,\n",
       "   \"max_position_embeddings\": 32768,\n",
       "   \"max_window_layers\": 24,\n",
       "   \"model_type\": \"qwen2\",\n",
       "   \"num_attention_heads\": 14,\n",
       "   \"num_hidden_layers\": 24,\n",
       "   \"num_key_value_heads\": 2,\n",
       "   \"quantization_config\": {\n",
       "     \"bnb_4bit_compute_dtype\": \"float16\",\n",
       "     \"bnb_4bit_quant_type\": \"nf4\",\n",
       "     \"bnb_4bit_use_double_quant\": true,\n",
       "     \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "     \"llm_int8_has_fp16_weight\": false,\n",
       "     \"llm_int8_skip_modules\": null,\n",
       "     \"llm_int8_threshold\": 6.0,\n",
       "     \"load_in_4bit\": true,\n",
       "     \"load_in_8bit\": false,\n",
       "     \"quant_method\": \"bitsandbytes\"\n",
       "   },\n",
       "   \"rms_norm_eps\": 1e-06,\n",
       "   \"rope_scaling\": null,\n",
       "   \"rope_theta\": 1000000.0,\n",
       "   \"sliding_window\": 32768,\n",
       "   \"tie_word_embeddings\": true,\n",
       "   \"torch_dtype\": \"float16\",\n",
       "   \"transformers_version\": \"4.42.4\",\n",
       "   \"unsloth_version\": \"2024.7\",\n",
       "   \"use_cache\": true,\n",
       "   \"use_sliding_window\": false,\n",
       "   \"vocab_size\": 151936\n",
       " },\n",
       " 'name_or_path': 'unsloth/Qwen2-0.5B-Instruct-bnb-4bit',\n",
       " 'warnings_issued': {},\n",
       " 'generation_config': GenerationConfig {\n",
       "   \"bos_token_id\": 151643,\n",
       "   \"do_sample\": true,\n",
       "   \"eos_token_id\": [\n",
       "     151645,\n",
       "     151643\n",
       "   ],\n",
       "   \"pad_token_id\": 151643,\n",
       "   \"repetition_penalty\": 1.1,\n",
       "   \"temperature\": 0.7,\n",
       "   \"top_k\": 20,\n",
       "   \"top_p\": 0.8\n",
       " },\n",
       " '_keep_in_fp32_modules': None,\n",
       " 'vocab_size': 151936,\n",
       " 'is_quantized': True,\n",
       " 'quantization_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
       " '_is_hf_initialized': True,\n",
       " '_old_forward': <bound method CausalLM_fast_forward.<locals>._CausalLM_fast_forward of Qwen2ForCausalLM(\n",
       "   (model): Qwen2Model(\n",
       "     (embed_tokens): Embedding(151936, 896)\n",
       "     (layers): ModuleList(\n",
       "       (0-23): 24 x Qwen2DecoderLayer(\n",
       "         (self_attn): Qwen2Attention(\n",
       "           (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "           (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm()\n",
       "         (post_attention_layernorm): Qwen2RMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       " )>,\n",
       " '_hf_hook': AlignDevicesHook(execution_device=0, offload=False, io_same_device=True, offload_buffers=False, place_submodules=True, skip_keys='past_key_values'),\n",
       " 'forward': functools.partial(<function add_hook_to_module.<locals>.new_forward at 0x7f8cef222dd0>, Qwen2ForCausalLM(\n",
       "   (model): Qwen2Model(\n",
       "     (embed_tokens): Embedding(151936, 896)\n",
       "     (layers): ModuleList(\n",
       "       (0-23): 24 x Qwen2DecoderLayer(\n",
       "         (self_attn): Qwen2Attention(\n",
       "           (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "           (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm()\n",
       "         (post_attention_layernorm): Qwen2RMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       " )),\n",
       " 'to': <function torch.nn.modules.module.Module.to(*args, **kwargs)>,\n",
       " 'cuda': <function torch.nn.modules.module.Module.cuda(device: Union[int, torch.device, NoneType] = None) -> ~T>,\n",
       " 'hf_device_map': {'': 0},\n",
       " 'is_loaded_in_4bit': True,\n",
       " 'is_4bit_serializable': True,\n",
       " 'hf_quantizer': <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer at 0x7f8cd5f69180>,\n",
       " 'max_seq_length': 32768,\n",
       " 'original_push_to_hub': <bound method PushToHubMixin.push_to_hub of Qwen2ForCausalLM(\n",
       "   (model): Qwen2Model(\n",
       "     (embed_tokens): Embedding(151936, 896)\n",
       "     (layers): ModuleList(\n",
       "       (0-23): 24 x Qwen2DecoderLayer(\n",
       "         (self_attn): Qwen2Attention(\n",
       "           (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "           (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm()\n",
       "         (post_attention_layernorm): Qwen2RMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       " )>,\n",
       " 'push_to_hub': <bound method unsloth_push_to_hub of Qwen2ForCausalLM(\n",
       "   (model): Qwen2Model(\n",
       "     (embed_tokens): Embedding(151936, 896)\n",
       "     (layers): ModuleList(\n",
       "       (0-23): 24 x Qwen2DecoderLayer(\n",
       "         (self_attn): Qwen2Attention(\n",
       "           (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "           (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm()\n",
       "         (post_attention_layernorm): Qwen2RMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       " )>,\n",
       " 'model_tags': ['unsloth'],\n",
       " 'push_to_hub_merged': <bound method unsloth_push_to_hub_merged of Qwen2ForCausalLM(\n",
       "   (model): Qwen2Model(\n",
       "     (embed_tokens): Embedding(151936, 896)\n",
       "     (layers): ModuleList(\n",
       "       (0-23): 24 x Qwen2DecoderLayer(\n",
       "         (self_attn): Qwen2Attention(\n",
       "           (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "           (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm()\n",
       "         (post_attention_layernorm): Qwen2RMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       " )>,\n",
       " 'save_pretrained_merged': <bound method unsloth_save_pretrained_merged of Qwen2ForCausalLM(\n",
       "   (model): Qwen2Model(\n",
       "     (embed_tokens): Embedding(151936, 896)\n",
       "     (layers): ModuleList(\n",
       "       (0-23): 24 x Qwen2DecoderLayer(\n",
       "         (self_attn): Qwen2Attention(\n",
       "           (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "           (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm()\n",
       "         (post_attention_layernorm): Qwen2RMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       " )>,\n",
       " 'push_to_hub_gguf': <bound method unsloth_push_to_hub_gguf of Qwen2ForCausalLM(\n",
       "   (model): Qwen2Model(\n",
       "     (embed_tokens): Embedding(151936, 896)\n",
       "     (layers): ModuleList(\n",
       "       (0-23): 24 x Qwen2DecoderLayer(\n",
       "         (self_attn): Qwen2Attention(\n",
       "           (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "           (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm()\n",
       "         (post_attention_layernorm): Qwen2RMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       " )>,\n",
       " 'save_pretrained_gguf': <bound method unsloth_save_pretrained_gguf of Qwen2ForCausalLM(\n",
       "   (model): Qwen2Model(\n",
       "     (embed_tokens): Embedding(151936, 896)\n",
       "     (layers): ModuleList(\n",
       "       (0-23): 24 x Qwen2DecoderLayer(\n",
       "         (self_attn): Qwen2Attention(\n",
       "           (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "           (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm()\n",
       "         (post_attention_layernorm): Qwen2RMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       " )>,\n",
       " 'push_to_hub_ggml': <bound method unsloth_convert_lora_to_ggml_and_push_to_hub of Qwen2ForCausalLM(\n",
       "   (model): Qwen2Model(\n",
       "     (embed_tokens): Embedding(151936, 896)\n",
       "     (layers): ModuleList(\n",
       "       (0-23): 24 x Qwen2DecoderLayer(\n",
       "         (self_attn): Qwen2Attention(\n",
       "           (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "           (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm()\n",
       "         (post_attention_layernorm): Qwen2RMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       " )>,\n",
       " 'save_pretrained_ggml': <bound method unsloth_convert_lora_to_ggml_and_save_locally of Qwen2ForCausalLM(\n",
       "   (model): Qwen2Model(\n",
       "     (embed_tokens): Embedding(151936, 896)\n",
       "     (layers): ModuleList(\n",
       "       (0-23): 24 x Qwen2DecoderLayer(\n",
       "         (self_attn): Qwen2Attention(\n",
       "           (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "           (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "           (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm()\n",
       "         (post_attention_layernorm): Qwen2RMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       " )>,\n",
       " '_saved_temp_tokenizer': Qwen2TokenizerFast(name_or_path='unsloth/Qwen2-0.5B-Instruct-bnb-4bit', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       " \t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " }}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gemma2.modeling_gemma2 import Gemma2ForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastTokenPooling(torch.nn.Module):\n",
    "    def forward(self, hidden_states, attn_mask=None):\n",
    "        if attn_mask is None:\n",
    "            features = hidden_states[:, -1]\n",
    "        else:\n",
    "            inds0 = torch.arange(hidden_states.size(0), device=hidden_states.device)\n",
    "            inds1 = attn_mask.sum(1).long() - 1\n",
    "            features = hidden_states[inds0, inds1]\n",
    "        return features\n",
    "\n",
    "\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "\n",
    "\n",
    "class FastLMForSequenceClassification(torch.nn.Module):\n",
    "    def __init__(self, model: PreTrainedModel, num_labels: int):\n",
    "        super().__init__()\n",
    "        assert hasattr(\n",
    "            model, \"lm_head\"\n",
    "        ), \"model must have an lm_head as unsloth support CausalLM model only\"\n",
    "        self.model = model\n",
    "\n",
    "        # remove the lm head and cleanup memory\n",
    "        del self.model.lm_head\n",
    "        import gc\n",
    "\n",
    "        for _ in range(3):\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # patch the forward method to not use lm head\n",
    "        from types import MethodType\n",
    "\n",
    "        self.model.forward = MethodType(_transformer_fast_forward, self.model)\n",
    "\n",
    "        self.pooler = LastTokenPooling()\n",
    "        self.score = torch.nn.Linear(\n",
    "            self.model.config.hidden_size,\n",
    "            num_labels,\n",
    "            bias=False,\n",
    "            dtype=torch.float32,\n",
    "            device=self.model.device,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # run through LLM\n",
    "        outputs: BaseModelOutputWithPast = self.model.forward(\n",
    "            input_ids=input_ids,\n",
    "            causal_mask=causal_mask,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # pool the last token\n",
    "        pooled_hidden_state = self.pooler(outputs.last_hidden_state, attention_mask)\n",
    "\n",
    "        # run through classifier\n",
    "        logits = self.score(pooled_hidden_state)\n",
    "\n",
    "        # add the loss computation here\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Qwen2 patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2070. Max memory: 7.607 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/.venv/ml/lib/python3.10/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Qwen2-0.5B-Instruct-bnb-4bit\"\n",
    ")\n",
    "\n",
    "model = FastLMForSequenceClassification(model, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[9707,   11, 1879,    0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: c10::Half != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencodings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[53], line 82\u001b[0m, in \u001b[0;36mFastLMForSequenceClassification.forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m pooled_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state, attention_mask)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# run through classifier\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooled_hidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# add the loss computation here\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.venv/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/ml/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: c10::Half != float"
     ]
    }
   ],
   "source": [
    "model(**encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
